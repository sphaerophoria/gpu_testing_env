From f2442d6711d1e4ed8a96d7844e426b9275b2c2f8 Mon Sep 17 00:00:00 2001
From: Mick Sayson <mick@sayson.com>
Date: Mon, 28 Oct 2024 20:03:35 +0000
Subject: [PATCH] Remove dead APIs

Previous experiments no longer do anything on the GPU, but APIs were
left in tact to avoid API churn. As we add more features, the old names
being here make things confusing
---
 drivers/gpu/drm/sphaero/sphaero_drv.c | 126 --------------------------
 include/uapi/drm/sphaero_drm.h        |  39 +-------
 2 files changed, 3 insertions(+), 162 deletions(-)

diff --git a/drivers/gpu/drm/sphaero/sphaero_drv.c b/drivers/gpu/drm/sphaero/sphaero_drv.c
index 49d090b2a..3e774197e 100644
--- a/drivers/gpu/drm/sphaero/sphaero_drv.c
+++ b/drivers/gpu/drm/sphaero/sphaero_drv.c
@@ -28,15 +28,8 @@ static const struct drm_driver driver;
 #define SPHAERO_SUPPORTED_BITS_PER_PIX 32
 #define SPHAERO_BYTES_PER_PIX (SPHAERO_SUPPORTED_BITS_PER_PIX / 8)
 
 enum sphaero_reg_cmd {
-	SPHAERO_REG_CMD_PUSH_FB_CHUNK,
-	SPHAERO_REG_CMD_COMMIT_FB,
-	SPHAERO_REG_CMD_SET_VB_SIZE,
-	SPHAERO_REG_CMD_PUSH_VB_CHUNK,
-	SPHAERO_REG_CMD_SET_TEXTURE_SIZE,
-	SPHAERO_REG_CMD_PUSH_TEXTURE_CHUNK,
-	SPHAERO_REG_CMD_PUSH_MODEL_TRANSFORM,
 	SPHAERO_REG_CMD_CREATE_GL_TEX,
 	SPHAERO_REG_CMD_GL_CLEAR,
 	SPHAERO_REG_CMD_SET_HW_FB,
 	SPHAERO_REG_CMD_ALLOC_HW_BUF,
@@ -162,72 +155,16 @@ static const struct drm_mode_config_funcs sphaero_drv_mode_funcs = {
 	.atomic_check = drm_atomic_helper_check,
 	.atomic_commit = drm_atomic_helper_commit,
 };
 
-static int sphaero_create_gem_shmem(struct drm_device* dev, struct drm_file* file_priv, size_t size, uint32_t* handle) {
-	struct drm_gem_shmem_object *shmem_obj = drm_gem_shmem_create(dev, size);
-	if (IS_ERR(shmem_obj)) {
-		return PTR_ERR(shmem_obj);
-	}
-
-	int rc = drm_gem_handle_create(file_priv, &shmem_obj->base, handle);
-
-	// Both error and non-error path have to release their reference
-	// to the created object, the handle owns it now. If the handle failed
-	// to create, then the object should get freed anyways... I think,
-	// I don't actually know
-	drm_gem_object_put(&shmem_obj->base);
-	return rc;
-}
-
 static const struct drm_plane_funcs sphaero_gpu_plane_funcs = {
 	.update_plane		= drm_atomic_helper_update_plane,
 	.disable_plane		= drm_atomic_helper_disable_plane,
 	.reset			= drm_atomic_helper_plane_reset,
 	.atomic_duplicate_state = drm_atomic_helper_plane_duplicate_state,
 	.atomic_destroy_state	= drm_atomic_helper_plane_destroy_state,
 };
 
-static int sphaero_do_gem_xfer(struct sphaero_priv* priv, struct drm_gem_object* gem_obj, enum sphaero_reg_cmd cmd, bool with_chunk_len) {
-	struct drm_gem_shmem_object* shmem_obj = container_of(gem_obj, struct drm_gem_shmem_object, base);
-	// It seems like we are leaking a locked resource here, and we kinda
-	// are, however the table is held as a member of shmem_obj and re-used
-	// on future mappings. It seems like the model here is that if we are
-	// mapping a gem for DMA, it should stay mapped for the lifetime of
-	// the gem handle. It will get unmapped when the object is freed
-	struct sg_table* sg_table = drm_gem_shmem_get_pages_sgt(shmem_obj);
-	if (IS_ERR(sg_table)) {
-		return PTR_ERR(sg_table);
-	}
-
-	struct scatterlist *sg;
-	int si;
-	for_each_sgtable_dma_sg(sg_table, sg, si) {
-		u64 dma_addr = sg_dma_address(sg);
-		u64 dma_length = sg_dma_len(sg);
-		priv->regs[1]  = cpu_to_le32(dma_addr);
-		priv->regs[2] = cpu_to_le32(dma_addr >> 32);
-		if (with_chunk_len) {
-			priv->regs[3]  = cpu_to_le32(dma_length);
-			priv->regs[4] = cpu_to_le32(dma_length >> 32);
-		}
-		priv->regs[0] = cmd;
-	}
-	return 0;
-}
-
-static void sphaero_gpu_send_elem_size(struct sphaero_priv* priv, enum sphaero_reg_cmd cmd, u64 size) {
-	priv->regs[1] = size;
-	priv->regs[2] = size >> 32;
-	priv->regs[0] = cmd;
-}
-
-
-static int sphaero_do_size_prefixed_gem_xfer(struct sphaero_priv* priv, struct drm_gem_object* gem_obj, enum sphaero_reg_cmd size_cmd, enum sphaero_reg_cmd chunk_cmd, uint64_t size) {
-
-	sphaero_gpu_send_elem_size(priv, size_cmd, size);
-	return sphaero_do_gem_xfer(priv, gem_obj, chunk_cmd, true);
-}
 
 struct drm_gem_sphaero_obj {
 	struct drm_gem_object base;
 	uint64_t hw_id;
@@ -528,14 +465,8 @@ static struct pci_driver sphaero_driver = {
 module_pci_driver(sphaero_driver);
 
 DEFINE_DRM_GEM_FOPS(sphaero_gpu_fops);
 
-static int sphaero_gpu_create_gpu_obj_ioctl(struct drm_device *dev, void *data,
-				struct drm_file *file) {
-	struct drm_sphaero_create_gpu_obj *params = data;
-	return sphaero_create_gem_shmem(dev, file, params->size, &params->handle);
-}
-
 static int sphaero_gpu_map_gpu_obj_ioctl(struct drm_device *dev, void *data,
 				struct drm_file *file) {
 	struct drm_sphaero_map_gpu_obj *params = data;
 
@@ -547,57 +478,8 @@ static int sphaero_gpu_map_gpu_obj_ioctl(struct drm_device *dev, void *data,
 	drm_gem_object_put(gobj);
 	return 0;
 }
 
-static int sphaero_gpu_upload_ioctl_sized(struct drm_device *dev, void *data,
-				struct drm_file *file, enum sphaero_reg_cmd size_cmd, enum sphaero_reg_cmd chunk_cmd) {
-	struct drm_sphaero_upload_gpu_obj *params = data;
-	struct sphaero_priv* priv = dev->dev_private;
-	struct drm_gem_object *gobj = drm_gem_object_lookup(file, params->handle);
-	if (!gobj) {
-		return -ENOENT;
-	}
-
-	int rc = sphaero_do_size_prefixed_gem_xfer(priv, gobj, size_cmd, chunk_cmd, params->size);
-	drm_gem_object_put(gobj);
-	return rc;
-}
-
-static int sphaero_gpu_upload_ioctl_unsized(struct drm_device *dev, void *data,
-				struct drm_file *file, enum sphaero_reg_cmd cmd) {
-	struct drm_sphaero_upload_gpu_obj *params = data;
-	struct sphaero_priv* priv = dev->dev_private;
-	struct drm_gem_object *gobj = drm_gem_object_lookup(file, params->handle);
-	if (!gobj) {
-		return -ENOENT;
-	}
-
-	int rc = sphaero_do_gem_xfer(priv, gobj, cmd, false);
-	drm_gem_object_put(gobj);
-	return rc;
-}
-
-static int sphaero_gpu_upload_vb_ioctl(struct drm_device *dev, void *data,
-				struct drm_file *file) {
-	return sphaero_gpu_upload_ioctl_sized(dev, data, file, SPHAERO_REG_CMD_SET_VB_SIZE, SPHAERO_REG_CMD_PUSH_VB_CHUNK);
-}
-
-static int sphaero_gpu_upload_texture_ioctl(struct drm_device *dev, void *data,
-                                            struct drm_file *file) {
-	return sphaero_gpu_upload_ioctl_sized(dev, data, file, SPHAERO_REG_CMD_SET_TEXTURE_SIZE, SPHAERO_REG_CMD_PUSH_TEXTURE_CHUNK);
-}
-
-static int sphaero_gpu_upload_transform_ioctl(struct drm_device *dev, void *data,
-                                              struct drm_file *file) {
-	struct drm_sphaero_upload_gpu_obj *params = data;
-	if (params->size != 16 * 4) {
-		return -EINVAL;
-	}
-
-	return sphaero_gpu_upload_ioctl_unsized(dev, data, file, SPHAERO_REG_CMD_PUSH_MODEL_TRANSFORM);
-}
-
-
 static int sphaero_gpu_create_gl_tex_ioctl(struct drm_device *dev, void *data,
 				struct drm_file *file) {
 	struct drm_gem_sphaero_obj *obj = drm_gem_sphaero_create(dev, 0 /*No way to map to userspace*/);
 	if (IS_ERR(obj)) {
@@ -640,18 +522,10 @@ static int sphaero_gpu_gl_clear_ioctl(struct drm_device *dev, void *data,
 	return 0;
 }
 
 struct drm_ioctl_desc sphaero_gpu_ioctls[DRM_SPHAERO_NUM_IOCTLS] = {
-	DRM_IOCTL_DEF_DRV(SPHAERO_CREATE_GPU_OBJ, sphaero_gpu_create_gpu_obj_ioctl,
-			  DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(SPHAERO_MAP_GPU_OBJ, sphaero_gpu_map_gpu_obj_ioctl,
 			  DRM_RENDER_ALLOW),
-	DRM_IOCTL_DEF_DRV(SPHAERO_UPLOAD_VB, sphaero_gpu_upload_vb_ioctl,
-			  DRM_RENDER_ALLOW),
-	DRM_IOCTL_DEF_DRV(SPHAERO_UPLOAD_TEXTURE, sphaero_gpu_upload_texture_ioctl,
-			  DRM_RENDER_ALLOW),
-	DRM_IOCTL_DEF_DRV(SPHAERO_UPLOAD_TRANSFORM, sphaero_gpu_upload_transform_ioctl,
-			  DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(SPHAERO_CREATE_GL_TEX, sphaero_gpu_create_gl_tex_ioctl,
 			  DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(SPHAERO_GL_CLEAR, sphaero_gpu_gl_clear_ioctl,
 			  DRM_RENDER_ALLOW),
diff --git a/include/uapi/drm/sphaero_drm.h b/include/uapi/drm/sphaero_drm.h
index 9060a758c..507a2c8cf 100644
--- a/include/uapi/drm/sphaero_drm.h
+++ b/include/uapi/drm/sphaero_drm.h
@@ -6,23 +6,11 @@
 #if defined(__cplusplus)
 extern "C" {
 #endif
 
-#define DRM_SPHAERO_CREATE_GPU_OBJ  0x00
-#define DRM_SPHAERO_MAP_GPU_OBJ  0x01
-#define DRM_SPHAERO_UPLOAD_VB  0x02
-#define DRM_SPHAERO_UPLOAD_TEXTURE  0x03
-#define DRM_SPHAERO_UPLOAD_TRANSFORM  0x04
-#define DRM_SPHAERO_CREATE_GL_TEX 0x05
-#define DRM_SPHAERO_GL_CLEAR 0x06
-
-struct drm_sphaero_create_gpu_obj {
-	// inputs
-	__u64 size;
-
-	// outputs
-	__u32 handle;
-};
+#define DRM_SPHAERO_MAP_GPU_OBJ  0x00
+#define DRM_SPHAERO_CREATE_GL_TEX 0x01
+#define DRM_SPHAERO_GL_CLEAR 0x02
 
 struct drm_sphaero_map_gpu_obj {
 	// inputs
 	__u32 handle;
@@ -30,13 +18,8 @@ struct drm_sphaero_map_gpu_obj {
 	// outputs
 	__u64 offset;
 };
 
-struct drm_sphaero_upload_gpu_obj {
-	__u64 size;
-	__u32 handle;
-};
-
 struct drm_sphaero_create_gl_tex {
 	// inputs
 	__u32 width;
 	__u32 height;
@@ -54,28 +37,12 @@ struct drm_sphaero_gl_clear {
 	__u32 miny;
 	__u32 maxy;
 };
 
-#define DRM_IOCTL_SPHAERO_CREATE_GPU_OBJ \
-	DRM_IOWR(DRM_COMMAND_BASE + DRM_SPHAERO_CREATE_GPU_OBJ,\
-		struct drm_sphaero_create_gpu_obj)
-
 #define DRM_IOCTL_SPHAERO_MAP_GPU_OBJ \
 	DRM_IOWR(DRM_COMMAND_BASE + DRM_SPHAERO_MAP_GPU_OBJ,\
 		struct drm_sphaero_map_gpu_obj)
 
-#define DRM_IOCTL_SPHAERO_UPLOAD_VB \
-	DRM_IOWR(DRM_COMMAND_BASE + DRM_SPHAERO_UPLOAD_VB,\
-		struct drm_sphaero_upload_gpu_obj)
-
-#define DRM_IOCTL_SPHAERO_UPLOAD_TEXTURE \
-	DRM_IOWR(DRM_COMMAND_BASE + DRM_SPHAERO_UPLOAD_TEXTURE,\
-		struct drm_sphaero_upload_gpu_obj)
-
-#define DRM_IOCTL_SPHAERO_UPLOAD_TRANSFORM \
-	DRM_IOWR(DRM_COMMAND_BASE + DRM_SPHAERO_UPLOAD_TRANSFORM,\
-		struct drm_sphaero_upload_gpu_obj)
-
 #define DRM_IOCTL_SPHAERO_CREATE_GL_TEX \
 	DRM_IOWR(DRM_COMMAND_BASE + DRM_SPHAERO_CREATE_GL_TEX,\
 		struct drm_sphaero_create_gl_tex)
 
-- 
2.44.1

